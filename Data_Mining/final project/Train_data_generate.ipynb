{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note:\n",
    "\n",
    "1. we dont handle out of vocabulary problem\n",
    "2. we split each single chinese word and punctuation, and keep english words and number stick together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/train_1_update.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-cd2777827e49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/train_1_update.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mfile_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8-sig'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdatas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\n--------------------\\n\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/train_1_pos_ner.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpos_ner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/train_1_update.txt'"
     ]
    }
   ],
   "source": [
    "# with open('./data/train_1_update.txt','r', encoding='utf8') as f:\n",
    "with open('./data/train_1_update.txt','r', encoding='utf8') as f:\n",
    "    file_text = f.read().encode('utf-8').decode('utf-8-sig')\n",
    "datas = file_text.split('\\n\\n--------------------\\n\\n')[:-1]\n",
    "with open('./data/train_1_pos_ner.pkl', 'rb') as f:\n",
    "    pos_ner = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/SampleData_pos_ner.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7e73c38a6986>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mfile_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8-sig'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdatas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\n--------------------\\n\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/SampleData_pos_ner.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mpos_ner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/SampleData_pos_ner.pkl'"
     ]
    }
   ],
   "source": [
    "with open('SampleData_deid.txt','r', encoding='utf8') as f:\n",
    "    file_text = f.read().encode('utf-8').decode('utf-8-sig')\n",
    "datas = file_text.split('\\n\\n--------------------\\n\\n')[:-1]\n",
    "with open('./data/SampleData_pos_ner.pkl', 'rb') as f:\n",
    "    pos_ner = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/development_1.txt','r', encoding='utf8') as f:\n",
    "    file_text = f.read().encode('utf-8').decode('utf-8-sig')\n",
    "datas = [i.split('\\n')[1] for i in file_text.split('\\n\\n--------------------\\n\\n')[:-1]]\n",
    "with open('./data/development_1_pos_ner.pkl', 'rb') as f:\n",
    "    pos_ner = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## origin data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data_by_word_split(data, with_label=True):\n",
    "    all_lines = []\n",
    "    label_count = {}\n",
    "\n",
    "    label_transform = {\n",
    "        '醫師：': 'DOC',\n",
    "        '個管師：': 'CAS',\n",
    "        '民眾：': 'PAT'\n",
    "    }\n",
    "\n",
    "    for data in datas:\n",
    "        article, *entity = data.split('\\n')\n",
    "\n",
    "        entity_parse = [[int(col) if i != 4 and i != 3 else col for i, col in enumerate(e.split('\\t'))] for e in entity[1:]]\n",
    "        split_article = re.sub(r\"((醫師：)|(民眾：)|(個管師：)|[^A-Za-z\\d ']|[A-Za-z\\d\\-#%&\\./+×]+)\", r'\\1 ', article).split()\n",
    "        \n",
    "        if with_label:\n",
    "            split_label = []\n",
    "            char_idx = 0\n",
    "            entity_idx = 0\n",
    "\n",
    "            for word in split_article:\n",
    "                full_label = label_transform[word] if word in label_transform else 'O'\n",
    "                if entity_idx < len(entity_parse):\n",
    "                    _, stidx, edidx, _, label = entity_parse[entity_idx]\n",
    "                    if stidx <= char_idx < edidx:\n",
    "                        prefix = 'B' if char_idx == stidx else 'I'\n",
    "                        full_label = f'{prefix}-{label}'\n",
    "                        if char_idx == edidx - len(word):\n",
    "                            entity_idx += 1\n",
    "\n",
    "                split_label.append(full_label)    \n",
    "                char_idx += len(word)\n",
    "                \n",
    "            ###\n",
    "            for k in split_label:\n",
    "                if k not in label_count:\n",
    "                    label_count[k] = 0\n",
    "                label_count[k] += 1\n",
    "            ###\n",
    "                \n",
    "        else:\n",
    "            split_label = ['O' for _ in range(len(split_article))]\n",
    "\n",
    "        all_lines.append(f'{\" \".join(split_article)}\\t{\" \".join(split_label)}')\n",
    "        \n",
    "    #with open('./train_data_mod.txt', 'w', encoding='utf8') as f:\n",
    "    #    f.write('\\n'.join(all_lines))\n",
    "    \n",
    "    return all_lines\n",
    "\n",
    "\n",
    "def parse_data_by_char(data, with_label=True):\n",
    "    all_lines = []\n",
    "    \n",
    "    for data in datas:\n",
    "        article, *entity = data.split('\\n')\n",
    "\n",
    "        entity_parse = [[int(col) if i != 4 and i != 3 else col for i, col in enumerate(e.split('\\t'))] for e in entity[1:]]\n",
    "        split_article = list(article)\n",
    "        split_label = ['O' for _ in range(len(split_article))]\n",
    "        \n",
    "        if with_label:\n",
    "            for _, stidx, edidx, _, label in entity_parse:\n",
    "                for idx in range(stidx, edidx):\n",
    "                    prefix = 'B' if idx == stidx else 'I'\n",
    "                    split_label[idx] = f'{prefix}-{label}'\n",
    "        all_lines.append(f'{\" \".join(split_article)}\\t{\" \".join(split_label)}')\n",
    "        \n",
    "    return all_lines\n",
    "\n",
    "\n",
    "def parse_data_by_sentence_with_neighbor(data, with_label=True, sep_limit=1, sep_chars={'，', '。'}, neighbor_len=100):\n",
    "    all_lines = []\n",
    "    \n",
    "    for aid, data in enumerate(datas):\n",
    "        article, *entity = data.split('\\n')\n",
    "        \n",
    "        # save all entity with idx\n",
    "        entity_idxs = {}\n",
    "        if with_label:\n",
    "            for e in entity[1:]:\n",
    "                _, stidx, edidx, _, tp = e.split('\\t')\n",
    "                for i in range(int(stidx), int(edidx)):\n",
    "                    prefix = 'B' if i == int(stidx) else 'I'\n",
    "                    full_label = f'{prefix}-{tp}'\n",
    "                    entity_idxs[i] = full_label\n",
    "                    \n",
    "        # iterate the article character \n",
    "        sid = 0\n",
    "        sep_count = 0\n",
    "        curr_text = []\n",
    "        curr_label = []\n",
    "        curr_neighbor = []\n",
    "\n",
    "        for idx, char in enumerate(article):\n",
    "            curr_text.append(char)\n",
    "            curr_label.append(entity_idxs[idx] if idx in entity_idxs else 'O')\n",
    "            \n",
    "            if char in sep_chars:\n",
    "                sep_count += 1\n",
    "            \n",
    "            ### one sentence ends ###\n",
    "            if (char in sep_chars and sep_count >= sep_limit) or idx == len(article)-1:\n",
    "                ### get post neighbor for this sentence ###\n",
    "                if idx != len(article)-1:\n",
    "                    curr_neighbor.append(article[idx+1:idx+1+(neighbor_len*2)])\n",
    "                \n",
    "                ### find substring of pre and post neighbor ###\n",
    "                if len(curr_neighbor) == 2:\n",
    "                    # for middle sentences\n",
    "                    pre_neighbor = curr_neighbor[0][-neighbor_len//2:]\n",
    "                    post_neighbor = curr_neighbor[1][:neighbor_len//2]\n",
    "                elif idx == len(article)-1:\n",
    "                    # for last sentence\n",
    "                    #pre_neighbor = curr_neighbor[0][-neighbor_len//2:]\n",
    "                    #post_neighbor = curr_neighbor[0][-neighbor_len:-neighbor_len//2]\n",
    "                    pre_neighbor = curr_neighbor[0][-neighbor_len:]\n",
    "                    post_neighbor = ''\n",
    "                else:\n",
    "                    # for first sentence\n",
    "                    #pre_neighbor = curr_neighbor[0][neighbor_len//2:neighbor_len]\n",
    "                    #post_neighbor = curr_neighbor[0][:neighbor_len//2]\n",
    "                    pre_neighbor = ''\n",
    "                    post_neighbor = curr_neighbor[0][:neighbor_len]\n",
    "                \n",
    "                ### record line result ###\n",
    "                all_lines.append(f'{\" \".join(curr_text)}\\t{\" \".join(curr_label)}\\t{\" \".join(pre_neighbor)}\\t{\" \".join(post_neighbor)}\\t{aid}\\t{sid}')\n",
    "                \n",
    "                ### re-init ###\n",
    "                curr_text = []\n",
    "                curr_label = []\n",
    "                curr_neighbor = []\n",
    "                sep_count = 0\n",
    "                sid += 1\n",
    "                \n",
    "                ### get pre neighbor for next sentence ###\n",
    "                if idx != len(article)-1:\n",
    "                    stidx = idx-(neighbor_len*2)+1\n",
    "                    stidx = 0 if stidx < 0 else stidx\n",
    "                    curr_neighbor.append(article[stidx:idx+1])\n",
    "    return all_lines\n",
    "\n",
    "\n",
    "def parse_data_by_sentence(data, with_label=True, sep_limit=3, sep_chars={'，', '。'}):\n",
    "    all_lines = []\n",
    "\n",
    "    for aid, data in enumerate(datas):\n",
    "        # get curr article and entity\n",
    "        article, *entity = data.split('\\n')\n",
    "        cpos = pos_ner['pos'][aid]\n",
    "        cner = pos_ner['ner'][aid]\n",
    "\n",
    "        # save all entity with idx\n",
    "        entity_idxs = {}\n",
    "        if with_label:\n",
    "            for e in entity[1:]:\n",
    "                _, stidx, edidx, _, tp = e.split('\\t')\n",
    "                for i in range(int(stidx), int(edidx)):\n",
    "                    prefix = 'B' if i == int(stidx) else 'I'\n",
    "                    full_label = f'{prefix}-{tp}'\n",
    "                    entity_idxs[i] = full_label\n",
    "\n",
    "        # iterate the article character \n",
    "        sid = 0\n",
    "        sep_count = 0\n",
    "        curr_text = []\n",
    "        curr_label = []\n",
    "        curr_pos = []\n",
    "        curr_ner = []\n",
    "\n",
    "        for idx, char in enumerate(article):\n",
    "            curr_text.append(char)\n",
    "            curr_label.append(entity_idxs[idx] if idx in entity_idxs else 'O')\n",
    "            curr_pos.append(cpos[idx] if idx in cpos else 'O')\n",
    "            curr_ner.append(cner[idx] if idx in cner else 'O')\n",
    "            \n",
    "            if char in sep_chars:\n",
    "                sep_count += 1\n",
    "            \n",
    "            if (char in sep_chars and sep_count >= sep_limit) or idx == len(article)-1:\n",
    "                all_lines.append(f'{\" \".join(curr_text)}\\t{\" \".join(curr_label)}\\t{\" \".join(curr_pos)}\\t{\" \".join(curr_ner)}\\t{aid}\\t{sid}')\n",
    "                curr_text = []\n",
    "                curr_label = []\n",
    "                curr_pos = []\n",
    "                curr_ner = []\n",
    "                sep_count = 0\n",
    "                sid += 1\n",
    "    \n",
    "    return all_lines   \n",
    "\n",
    "\n",
    "def parse_data_by_sentence_into_bio(data, with_label=True, sep_limit=3, sep_chars={'，', '。'}):\n",
    "    all_lines = []\n",
    "    \n",
    "    for aid, data in enumerate(datas):\n",
    "        # get curr article and entity\n",
    "        article, *entity = data.split('\\n')\n",
    "\n",
    "        # save all entity with idx\n",
    "        entity_idxs = {}\n",
    "        if with_label:\n",
    "            for e in entity[1:]:\n",
    "                _, stidx, edidx, _, tp = e.split('\\t')\n",
    "                for i in range(int(stidx), int(edidx)):\n",
    "                    prefix = 'B' if i == int(stidx) else 'I'\n",
    "                    full_label = f'{prefix}-{tp}'\n",
    "                    entity_idxs[i] = full_label\n",
    "\n",
    "        # iterate the article character \n",
    "        sid = 0\n",
    "        sep_count = 0\n",
    "        curr_line = []\n",
    "\n",
    "        for idx, char in enumerate(article):\n",
    "            curr_line.append(f'{char} {entity_idxs[idx] if idx in entity_idxs else \"O\"} {aid}')\n",
    "            \n",
    "            if char in sep_chars:\n",
    "                sep_count += 1\n",
    "            \n",
    "            if (char in sep_chars and sep_count > sep_limit) or idx == len(article)-1:\n",
    "                all_lines.append('\\n'.join(curr_line))\n",
    "                curr_line = []\n",
    "                sep_count = 0\n",
    "                sid += 1\n",
    "    \n",
    "    return all_lines\n",
    "\n",
    "\n",
    "def parse_data_by_sentence_into_bio_with_max_len(data, with_label=True, max_len=400):\n",
    "    all_lines = []\n",
    "\n",
    "    for aid, data in enumerate(datas):\n",
    "        # get curr article and entity\n",
    "        article, *entity = data.split('\\n')\n",
    "\n",
    "        # save all entity with idx\n",
    "        entity_idxs = {}\n",
    "        if with_label:\n",
    "            for e in entity[1:]:\n",
    "                _, stidx, edidx, _, tp = e.split('\\t')\n",
    "                for i in range(int(stidx), int(edidx)):\n",
    "                    prefix = 'B' if i == int(stidx) else 'I'\n",
    "                    full_label = f'{prefix}-{tp}'\n",
    "                    entity_idxs[i] = full_label\n",
    "\n",
    "        # find all breakpoints\n",
    "        np_article = np.array(list(article))\n",
    "        is_split, = np.where(((np_article == '，') + (np_article == '。') + (np_article == '？') + (np_article == '！') + (np_article == ',')) == True)        \n",
    "        #is_split += 1\n",
    "        \n",
    "        # iterate the article character \n",
    "        word_count = 0\n",
    "        total_count = 0\n",
    "        curr_line = []\n",
    "\n",
    "        for idx, char in enumerate(article):\n",
    "            curr_line.append(f'{char} {entity_idxs[idx] if idx in entity_idxs else \"O\"} {aid}')\n",
    "            word_count += 1\n",
    "            \n",
    "            # if article ends\n",
    "            if idx == len(article)-1:\n",
    "                all_lines.append('\\n'.join(curr_line))\n",
    "                curr_line = []\n",
    "                word_count = 0\n",
    "            \n",
    "            # if is breakpoint\n",
    "            elif len(is_split) and idx == is_split[0]:\n",
    "                nxt_sentence_len = (is_split[1] if len(is_split) > 1 else len(article)) - is_split[0]\n",
    "                if word_count + nxt_sentence_len > max_len:\n",
    "                    all_lines.append('\\n'.join(curr_line))\n",
    "                    curr_line = []\n",
    "                    word_count = 0\n",
    "                is_split = is_split[1:]\n",
    "        #break\n",
    "        \n",
    "    return all_lines\n",
    "\n",
    "\n",
    "def parse_data_by_sentence_with_max_len(data, with_label=True, max_len=400):\n",
    "    all_lines = []\n",
    "\n",
    "    for aid, data in enumerate(datas):\n",
    "        # get curr article and entity\n",
    "        article, *entity = data.split('\\n')\n",
    "        cpos = pos_ner['pos'][aid]\n",
    "        cner = pos_ner['ner'][aid]\n",
    "\n",
    "        # save all entity with idx\n",
    "        entity_idxs = {}\n",
    "        if with_label:\n",
    "            for e in entity[1:]:\n",
    "                _, stidx, edidx, _, tp = e.split('\\t')\n",
    "                for i in range(int(stidx), int(edidx)):\n",
    "                    prefix = 'B' if i == int(stidx) else 'I'\n",
    "                    full_label = f'{prefix}-{tp}'\n",
    "                    entity_idxs[i] = full_label\n",
    "\n",
    "        # find all breakpoints\n",
    "        np_article = np.array(list(article))\n",
    "        is_split, = np.where(((np_article == '，') + (np_article == '。') + (np_article == '？') + (np_article == '！') + (np_article == ',')) == True)\n",
    "        \n",
    "        # iterate the article character \n",
    "        word_count = 0\n",
    "        sid = 0\n",
    "        curr_text = []\n",
    "        curr_label = []\n",
    "        curr_pos = []\n",
    "        curr_ner = []\n",
    "\n",
    "        for idx, char in enumerate(article):\n",
    "            curr_text.append(char)\n",
    "            curr_label.append(entity_idxs[idx] if idx in entity_idxs else 'O')\n",
    "            curr_pos.append(cpos[idx] if idx in cpos else 'O')\n",
    "            curr_ner.append(cner[idx] if idx in cner else 'O')\n",
    "            word_count += 1\n",
    "            \n",
    "            # if article ends\n",
    "            if idx == len(article)-1:\n",
    "                all_lines.append(f'{\" \".join(curr_text)}\\t{\" \".join(curr_label)}\\t{\" \".join(curr_pos)}\\t{\" \".join(curr_ner)}\\t{aid}\\t{sid}')\n",
    "                curr_text = []\n",
    "                curr_label = []\n",
    "                curr_pos = []\n",
    "                curr_ner = []\n",
    "                word_count = 0\n",
    "                sid += 1\n",
    "            \n",
    "            # if is breakpoint\n",
    "            elif len(is_split) and idx == is_split[0]:\n",
    "                nxt_sentence_len = (is_split[1] if len(is_split) > 1 else len(article)) - is_split[0]\n",
    "                if word_count + nxt_sentence_len > max_len:\n",
    "                    all_lines.append(f'{\" \".join(curr_text)}\\t{\" \".join(curr_label)}\\t{\" \".join(curr_pos)}\\t{\" \".join(curr_ner)}\\t{aid}\\t{sid}')\n",
    "                    curr_text = []\n",
    "                    curr_label = []\n",
    "                    curr_pos = []\n",
    "                    curr_ner = []\n",
    "                    word_count = 0\n",
    "                    sid += 1\n",
    "                is_split = is_split[1:]\n",
    "        #break\n",
    "        \n",
    "    return all_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split data with sentence + neighbor data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbor_len = 20\n",
    "all_lines = parse_data_by_sentence_with_neighbor(data, with_label=True, sep_limit=1, \n",
    "                                                 sep_chars={'，', '。', '？', '?', '!', '！'}, neighbor_len=neighbor_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../bert-sequence-tagging/TEA/test_data_sen_sep1_neigh{neighbor_len}.txt', 'w', encoding='utf8') as f:\n",
    "    f.write('\\n'.join(all_lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split data with max len sentence split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lines = parse_data_by_sentence_with_max_len(data, with_label=True, max_len=125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../bert-sequence-tagging/TEA/test_deid_sep_maxlen_125.txt', 'w', encoding='utf8') as f:\n",
    "    f.write('\\n'.join(all_lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split data into bio with max len sentence split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lines = parse_data_by_sentence_into_bio_with_max_len(data, with_label=True, max_len=140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../mrc-for-flat-nested-ner/data_preprocess/example/dev_deid_sep_maxlen_140.bio', 'w', encoding='utf8') as f:\n",
    "    f.write('\\n\\n'.join(all_lines) + '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split data into bio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lines = parse_data_by_sentence_into_bio(data, with_label=True, sep_limit=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../mrc-for-flat-nested-ner/data_preprocess/example/train_deid_sep_3.bio', 'w', encoding='utf8') as f:\n",
    "    f.write('\\n\\n'.join(all_lines) + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split data with sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lines = parse_data_by_sentence(datas, with_label=True, sep_limit=1, sep_chars={'。', '？', '?', '!', '！', '，'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(f'../bert-sequence-tagging/TEA/train_data_sen_sep_5_ext.txt', 'w', encoding='utf8') as f:\n",
    "#    f.write('\\n'.join(all_lines))\n",
    "    \n",
    "with open(f'../bert-sequence-tagging/TEA/dev_data_sen_sep_1_ext.txt', 'w', encoding='utf8') as f:\n",
    "    f.write('\\n'.join(all_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_count = [len(i.split('\\t')[0].split()) for i in all_lines]\n",
    "mean = np.mean(len_count)\n",
    "std = np.std(len_count)\n",
    "max_ = np.max(len_count)\n",
    "min_ = np.min(len_count)\n",
    "\n",
    "x = [i+1 for i in range(min_, max_+1)]\n",
    "y = [0 for i in range(min_, max_+1)]\n",
    "for c in len_count:\n",
    "    y[c-min_] += 1\n",
    "    \n",
    "plt.plot(x, y)\n",
    "plt.xlabel('length')\n",
    "plt.title(f'{mean:.4f} / {std:.4f} / {max_} / {min_}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, ele in enumerate(len_count):\n",
    "    if ele > 128:\n",
    "        print(ele, all_lines[idx].split('\\t')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split data with max length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for training data\n",
    "#all_lines = parse_data_by_word_split(datas, with_label=True)\n",
    "#all_lines = parse_data_by_char(datas, with_label=True)\n",
    "\n",
    "# for testing data\n",
    "#all_lines = parse_data_by_word_split(datas, with_label=False)\n",
    "all_lines = parse_data_by_char(datas, with_label=False)\n",
    "\n",
    "all_sublines = []\n",
    "max_seq_len = 510 #127 #511 #255\n",
    "\n",
    "for article_id, line in enumerate(all_lines):\n",
    "    curr_segment = []\n",
    "    article_list, label_list = [_.split() for _ in line.split('\\t')]\n",
    "    #max_segment = int(len(label_list)/max_seq_len) + 1\n",
    "    max_segment = math.ceil(len(label_list)/max_seq_len)\n",
    "    for s in range(max_segment):\n",
    "        curr_a = article_list[s*max_seq_len:(s+1)*max_seq_len]\n",
    "        curr_l = label_list[s*max_seq_len:(s+1)*max_seq_len]\n",
    "        curr_segment.append(f'{\" \".join(curr_a)}\\t{\" \".join(curr_l)}\\t{article_id}\\t{s}')\n",
    "    all_sublines.append('\\n'.join(curr_segment))\n",
    "    \n",
    "#with open(f'../bert-sequence-tagging/TEA/train_data_char_sub_{max_seq_len}.txt', 'w', encoding='utf8') as f:\n",
    "#    f.write('\\n'.join(all_sublines))\n",
    "    \n",
    "with open(f'../bert-sequence-tagging/TEA/dev_data_char_sub_{max_seq_len}.txt', 'w', encoding='utf8') as f:\n",
    "    f.write('\\n'.join(all_sublines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lines[0].split('\\t')[0].split('。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split data with conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_conversation = []\n",
    "\n",
    "for line in all_lines:\n",
    "    curr_conversation = []\n",
    "    char_idx = 0\n",
    "    \n",
    "    article_list, label_list = [_.split() for _ in line.split('\\t')]\n",
    "    np_label_list = np.array(label_list)\n",
    "    is_speaker, = np.where(((np_label_list == 'DOC') + (np_label_list == 'PAT') + (np_label_list == 'CAS')) == True)\n",
    "    \n",
    "    for idx in range(len(is_speaker)-1):\n",
    "        stidx, edidx = is_speaker[idx], is_speaker[idx+1]\n",
    "        curr_conversation.append((article_list[stidx:edidx], label_list[stidx:edidx]))\n",
    "    \n",
    "    all_conversation.append(curr_conversation)\n",
    "    \n",
    "with open('./train_data_conversation.pkl', 'wb') as f:\n",
    "    pickle.dump(all_conversation, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find max conversation in one training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lines_short = []\n",
    "max_seq_len = 128\n",
    "\n",
    "word_count = 0\n",
    "curr_a = []\n",
    "curr_l = []\n",
    "\n",
    "for aid, curr_conversation in enumerate(all_conversation):\n",
    "    for cid, (a, l) in enumerate(curr_conversation):\n",
    "        curr_a.extend(a)\n",
    "        \n",
    "        curr_l.extend(l)\n",
    "        word_count += len(l)\n",
    "        \n",
    "        if cid == len(curr_conversation)-1 or word_count + len(curr_conversation[cid+1][0]) > max_seq_len:\n",
    "            all_lines_short.append(f'{\" \".join(curr_a)}\\t{\" \".join(curr_l)}')\n",
    "            word_count = 0\n",
    "            curr_a = []\n",
    "            curr_l = []\n",
    "        \n",
    "with open('./train_data_short.txt', 'w', encoding='utf8') as f:\n",
    "    f.write('\\n'.join(all_lines_short))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = int(len(all_lines_short)*0.7)\n",
    "\n",
    "with open('../bert-sequence-tagging/TEA/train_row__.txt', 'w', encoding='utf8') as f:\n",
    "    f.write('\\n'.join(all_lines_short[:train_test_split]))\n",
    "    \n",
    "with open('../bert-sequence-tagging/TEA/dev_row__.txt', 'w', encoding='utf8') as f:\n",
    "    f.write('\\n'.join(all_lines_short[train_test_split:]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow3",
   "language": "python",
   "name": "tensorflow3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
